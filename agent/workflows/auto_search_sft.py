import argparse
import asyncio
import json
import logging
import os
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Union

# Configure logging for filter messages to appear in console and file
def setup_mcp_logging(log_dir: Optional[Path] = None) -> Optional[Path]:
    """Setup file logging for MCP-related actions (search, filtering, browse).
    
    Args:
        log_dir: Directory to write log file. If None, uses 'logs' directory in agent root.
        
    Returns:
        Path to the log file, or None if setup failed.
    """
    if log_dir is None:
        # Use logs directory in agent root
        agent_root = Path(__file__).parent.parent
        log_dir = agent_root / "logs"
    
    # Create logs directory if it doesn't exist
    log_dir.mkdir(parents=True, exist_ok=True)
    
    # Create log file with timestamp
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = log_dir / f"mcp_actions_{timestamp}.log"
    
    # Create file handler with detailed format
    file_handler = logging.FileHandler(log_file, mode='a', encoding='utf-8')
    file_handler.setLevel(logging.INFO)
    file_formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    file_handler.setFormatter(file_formatter)
    
    # Create console handler
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    console_handler.setFormatter(console_formatter)
    
    # Configure root logger with both handlers
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO)
    
    # Remove existing handlers to avoid duplicates
    root_logger.handlers.clear()
    
    # Add both handlers
    root_logger.addHandler(file_handler)
    root_logger.addHandler(console_handler)
    
    # Configure specific loggers for MCP-related modules
    mcp_loggers = [
        'dr_agent.tool_interface.mcp_tools',
        'dr_agent.filters.cochrane',
        'dr_agent.filters.base',
    ]
    
    for logger_name in mcp_loggers:
        logger = logging.getLogger(logger_name)
        logger.setLevel(logging.INFO)
        logger.propagate = True  # Propagate to root logger
    
    return log_file

# Setup logging
mcp_log_file = setup_mcp_logging()
if mcp_log_file:
    print(f"üìù MCP actions will be logged to: {mcp_log_file}")

import dotenv
from dr_agent.agent_interface import BaseAgent
from dr_agent.client import DocumentToolOutput, LLMToolClient, ToolOutput
from dr_agent.shared_prompts import UNIFIED_TOOL_CALLING_STRUCTURED_PROMPTS
from dr_agent.tool_interface.chained_tool import ChainedTool
from dr_agent.tool_interface.mcp_tools import (
    BaseTool,
    Crawl4AIBrowseTool,
    JinaBrowseTool,
    SemanticScholarSnippetSearchTool,
    SerperBrowseTool,
    SerperSearchTool,
)
from dr_agent.filters import CochraneResultFilter
from dr_agent.utils import (
    check_port,
    extract_port_from_url,
    launch_mcp_server,
    launch_vllm_server,
)
from dr_agent.workflow import BaseWorkflow, BaseWorkflowConfiguration
from rich.console import Console
from rich.panel import Panel
from rich.prompt import Confirm
from rich.table import Table

# Make sure the .env file is in the root directory of the project rl-rag-mcp/.env
dotenv.load_dotenv(Path(__file__).parent.parent.parent / ".env")


@dataclass
class WebPageReaderAgentV2(BaseAgent):
    question: Optional[str] = None
    prompt = """
We are searching on the internet for the following question:
{question}
Here is some webpage scraped from the internet:
{document}
Can you clean the raw webpage text and convert it into a more readable format? You should remove all the unnecessary information and keep the main content of the page. Please produce the output in the format of "Cleaned webpage text:\n[you text here]".
""".strip()

    def preprocess_input(self, documents: Union[str, Any]) -> Dict[str, str]:
        # Accept either a raw string or a ToolOutput-like object with an `output` attribute
        assert self.question is not None, "Question is not set"

        if isinstance(documents, DocumentToolOutput):
            # print("using DocumentToolOutput")
            doc_str = "\n".join(
                [
                    document.simple_stringify()[: 32000 * 4 // len(documents.documents)]
                    for document in documents.documents
                ]
            )
        elif hasattr(documents, "output"):
            doc_str = documents.output
        else:
            doc_str = documents if isinstance(documents, str) else str(documents)
        input_params = {"question": self.question, "document": doc_str}
        # print(input_params)
        return input_params

    def postprocess_output(self, result: Dict[str, Any]) -> str:
        output_string = result.generated_text
        if "</think>" in output_string:
            output_string = "".join(output_string.split("</think>")[1:]).strip()

        if "Cleaned webpage text:" in output_string:
            output_string = output_string.split("Cleaned webpage text:")[1].strip()

        return output_string


@dataclass
class SearchAgent(BaseAgent):
    prompt_version: str = "v20250907"
    use_research_assistant_prompt: bool = False

    def prompt(
        self,
        question: str,
        dataset_name: Optional[str] = None,
        history: Optional[List[Dict[str, str]]] = None,
    ) -> str:

        PROMPT = UNIFIED_TOOL_CALLING_STRUCTURED_PROMPTS[self.prompt_version]
        system_prompt = PROMPT["system_prompt"]
        
        # Use Research Assistant prompt if enabled
        if self.use_research_assistant_prompt:
            from dr_agent.prompts import RESEARCH_ASSISTANT_PROMPT
            system_prompt = RESEARCH_ASSISTANT_PROMPT

        if dataset_name in [
            "2wiki",
            "simpleqa",
            "browsecomp",
            "bc_synthetic_depth_one_v2_verified",
            "bc_synthetic_varied_depth_o3_verified",
            "webwalker",
            "hle",
            "dsqa",
        ]:
            instruction_field_name = "exact_answer"
        elif dataset_name in ["sqav2", "genetic_diseases_qa"]:
            instruction_field_name = "long_form"
        elif dataset_name in ["healthbench", "deep_research_bench", "researchqa"]:
            instruction_field_name = "short_form"
        elif dataset_name and "sft-mix" in dataset_name:
            if "short_form" in dataset_name:
                instruction_field_name = "exact_answer"
            elif "long_form" in dataset_name:
                instruction_field_name = "long_form"  # or "short_form"?
            else:
                raise ValueError(
                    f"Unclear which instruction field name to use for the sft mix dataset: {dataset_name}"
                )
        else:
            if "short_form" in str(dataset_name):
                instruction_field_name = "exact_answer"
            elif "long_form" in str(dataset_name):
                instruction_field_name = "long_form"
            else:
                print("set additional instructions none")
                instruction_field_name = None

        messages = [
            {
                "role": "system",
                "content": system_prompt,
            }
        ]

        if history:
            messages.extend(history)

        messages.append(
            {
                "role": "user",
                "content": (
                    question
                    + "\n\n"
                    + PROMPT["additional_instructions"][instruction_field_name]
                    if instruction_field_name is not None
                    else question
                ),
            }
        )

        return messages

    def postprocess_output(self, result: Dict[str, Any]) -> str:
        output_string = result.generated_text
        if "</think>" in output_string:
            output_string = "".join(output_string.split("</think>")[1:]).strip()

        if "<answer>" in output_string:
            output_string = (
                output_string.split("<answer>")[1].split("</answer>")[0].strip()
            )

        # Replace the "\boxed{" with "\\boxed{"
        output_string = output_string.replace("\boxed{", "\\boxed{")

        if "\\boxed{" in output_string:
            output_string = output_string.split("\\boxed{")[1].split("}")[0].strip()

        return output_string


@dataclass
class AnswerAgent(BaseAgent):
    prompt_version: str = "v20250907"

    def prompt(self, question: str, history: str, dataset_name: str) -> str:

        PROMPT = UNIFIED_TOOL_CALLING_STRUCTURED_PROMPTS[self.prompt_version]
        if dataset_name in [
            "2wiki",
            "simpleqa",
            "browsecomp",
            "bc_synthetic_depth_one_v2_verified",
            "bc_synthetic_varied_depth_o3_verified",
            "webwalker",
        ]:
            instruction_field_name = "exact_answer"
        elif dataset_name in ["sqav2", "genetic_diseases_qa"]:
            instruction_field_name = "long_form"
        elif dataset_name in ["healthbench", "deep_research_bench", "researchqa"]:
            instruction_field_name = "short_form"
        else:
            raise ValueError(f"Invalid dataset name: {dataset_name}")

        return [
            {
                "role": "system",
                "content": PROMPT["system_prompt"],
            },
            {
                "role": "user",
                "content": question
                + "\n\n"
                + PROMPT["additional_instructions"][instruction_field_name],
            },
            {
                "role": "assistant",
                "content": history,
            },
            {
                "role": "user",
                "content": "Now please generate an answer based on the search results by far.",
            },
        ]

    def postprocess_output(self, result: Dict[str, Any]) -> str:
        output_string = result.generated_text
        if "</think>" in output_string:
            output_string = "".join(output_string.split("</think>")[1:]).strip()

        if "<answer>" in output_string:
            output_string = (
                output_string.split("<answer>")[1].split("</answer>")[0].strip()
            )

        # Replace the "\boxed{" with "\\boxed{"
        output_string = output_string.replace("\boxed{", "\\boxed{")

        if "\\boxed{" in output_string:
            output_string = output_string.split("\\boxed{")[1].split("}")[0].strip()

        return output_string


class NoBrowseTool(BaseTool):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    async def __call__(self, *args, **kwargs):
        return DocumentToolOutput(
            output="Browse tool is not available at this time. Please try other tools.",
            called=True,
            timeout=False,
            runtime=0.0,
            error=None,
            call_id=self._generate_call_id(),
            raw_output=None,
            documents=[],
            tool_name="no_browse",
        )

    def _format_output(self, output: ToolOutput) -> str:
        return output.output

    def _generate_tool_schema(self):
        return {
            "type": "object",
            "properties": {"url": {"type": "string", "description": "URL to browse"}},
            "required": ["url"],
        }


class AutoReasonSearchWorkflow(BaseWorkflow):
    _default_configuration_path = os.path.join(
        os.path.dirname(__file__), "auto_search_sft.yaml"
    )

    class Configuration(BaseWorkflowConfiguration):

        tool_parser: str

        search_tool_name: str = "serper"

        # Separate generation client (SFT model)
        search_agent_base_url: Optional[str] = None
        search_agent_model_name: str = "dr-tulu/DR-Tulu-8B"
        search_agent_tokenizer_name: str = "Qwen/Qwen3-8B"
        search_agent_api_key: str = "dummy-key"
        search_agent_max_tokens: int = 32000
        search_agent_temperature: float = 0.7
        search_agent_max_tool_calls: int = 10

        use_browse_agent: bool = False
        browse_agent_base_url: Optional[str] = None
        browse_agent_model_name: str = "Qwen/Qwen3-8B"
        browse_agent_tokenizer_name: str = "Qwen/Qwen3-8B"
        browse_agent_api_key: str = "dummy-key"
        browse_agent_max_tokens: int = 32000
        browse_agent_temperature: float = 0.3

        # MCP transport configuration
        mcp_transport_type: str = "StreamableHttpTransport"
        mcp_executable: Optional[str] = None
        mcp_port: int = 8000

        # Search configuration
        number_documents_to_search: int = 10
        search_timeout: int = 60

        # Browse configuration
        browse_tool_name: Optional[str] = "crawl4ai"
        browse_timeout: int = 60
        browse_max_pages_to_fetch: int = 10
        browse_context_char_length: int = 6000
        crawl4ai_use_docker_version: bool = False
        crawl4ai_use_ai2_config: bool = False

        prompt_version: str = "v20250907"
        
        # Filtering configuration
        enable_filtering: bool = False
        filter_title_list: Optional[List[str]] = None
        filter_title_list_file: Optional[str] = None  # Path to JSON file with titles
        filter_source_title: Optional[str] = None
        filter_publication_date: Optional[str] = None
        use_research_assistant_prompt: bool = False

    def before_launch_check(self) -> None:
        """Check if MCP server and vLLM servers are running, launch if needed."""
        cfg = self.configuration
        if cfg is None:
            return

        console = Console()

        console.print()
        console.print(Panel.fit("üîç Service Check", style="bold cyan"))
        console.print()

        # Check MCP server
        mcp_port = getattr(cfg, "mcp_port", 8000)
        if not check_port(mcp_port):
            console.print(
                f"[yellow]‚ö†[/yellow]  MCP server is not running on port [bold]{mcp_port}[/bold]"
            )
            if Confirm.ask("Launch MCP server?"):
                # Get the centralized log file from root logger
                root_logger = logging.getLogger()
                mcp_log_file = None
                for handler in root_logger.handlers:
                    if isinstance(handler, logging.FileHandler):
                        mcp_log_file = Path(handler.baseFilename)
                        break
                
                process = launch_mcp_server(mcp_port, self.logger, log_file=mcp_log_file)
                if process:
                    self._launched_processes.append(process)
                    console.print(
                        f"[green]‚úì[/green]  MCP server launched on port {mcp_port}"
                    )
                else:
                    console.print(
                        "[red]‚úó[/red]  Failed to start MCP server", style="bold red"
                    )
                    raise RuntimeError(
                        "Failed to start MCP server. Please launch it manually."
                    )
            else:
                console.print("[red]‚úó[/red]  MCP server is required", style="bold red")
                raise RuntimeError(
                    "MCP server is required. Please launch it manually or allow automatic launch."
                )
        else:
            console.print(
                f"[green]‚úì[/green]  MCP server is running on port [bold]{mcp_port}[/bold]"
            )

        # Check search agent vLLM server
        search_base_url = getattr(cfg, "search_agent_base_url", None)
        if search_base_url:
            port = extract_port_from_url(search_base_url)
            if port and not check_port(port):
                console.print(
                    f"[yellow]‚ö†[/yellow]  Search agent vLLM server is not running on port [bold]{port}[/bold]"
                )
                search_model = getattr(cfg, "search_agent_model_name", None)
                if search_model:
                    if Confirm.ask(
                        f"Launch vLLM server for [cyan]{search_model}[/cyan] on port {port}?"
                    ):
                        process = launch_vllm_server(
                            search_model, port, gpu_id=0, logger=self.logger
                        )
                        if process:
                            self._launched_processes.append(process)
                            console.print(
                                f"[green]‚úì[/green]  vLLM server launched for {search_model} on port {port}"
                            )
                        else:
                            console.print(
                                f"[yellow]‚ö†[/yellow]  Failed to start vLLM server. Manual launch command:"
                            )
                            console.print(
                                f"   [dim]CUDA_VISIBLE_DEVICES=0 vllm serve {search_model} --port {port} --dtype auto --max-model-len 40960[/dim]"
                            )
                    else:
                        console.print(f"[blue]üí°[/blue]  Manual launch command:")
                        console.print(
                            f"   [dim]CUDA_VISIBLE_DEVICES=0 vllm serve {search_model} --port {port} --dtype auto --max-model-len 40960[/dim]"
                        )
            elif port:
                console.print(
                    f"[green]‚úì[/green]  Search agent vLLM server is accessible on port [bold]{port}[/bold]"
                )

        # Check browse agent vLLM server if enabled
        use_browse_agent = getattr(cfg, "use_browse_agent", False)
        if use_browse_agent:
            browse_base_url = getattr(cfg, "browse_agent_base_url", None)
            if browse_base_url:
                port = extract_port_from_url(browse_base_url)
                if port and not check_port(port):
                    console.print(
                        f"[yellow]‚ö†[/yellow]  Browse agent vLLM server is not running on port [bold]{port}[/bold]"
                    )
                    browse_model = getattr(cfg, "browse_agent_model_name", None)
                    if browse_model:
                        if Confirm.ask(
                            f"Launch vLLM server for [cyan]{browse_model}[/cyan] on port {port}?"
                        ):
                            process = launch_vllm_server(
                                browse_model, port, gpu_id=1, logger=self.logger
                            )
                            if process:
                                self._launched_processes.append(process)
                                console.print(
                                    f"[green]‚úì[/green]  vLLM server launched for {browse_model} on port {port}"
                                )
                            else:
                                console.print(
                                    f"[yellow]‚ö†[/yellow]  Failed to start vLLM server. Manual launch command:"
                                )
                                console.print(
                                    f"   [dim]CUDA_VISIBLE_DEVICES=1 vllm serve {browse_model} --port {port} --dtype auto --max-model-len 40960[/dim]"
                                )
                        else:
                            console.print(f"[blue]üí°[/blue]  Manual launch command:")
                            console.print(
                                f"   [dim]CUDA_VISIBLE_DEVICES=1 vllm serve {browse_model} --port {port} --dtype auto --max-model-len 40960[/dim]"
                            )
                elif port:
                    console.print(
                        f"[green]‚úì[/green]  Browse agent vLLM server is accessible on port [bold]{port}[/bold]"
                    )

        console.print()
        console.print(Panel.fit("‚úÖ Service Check Complete", style="bold green"))
        console.print()

    def setup_components(
        self,
        mcp_transport_type: Optional[str] = "StreamableHttpTransport",
        mcp_executable: Optional[str] = None,
        mcp_port: Optional[int] = 8000,
    ) -> None:
        cfg = self.configuration
        assert cfg is not None
        # print(cfg)

        # Allow configuration overrides for MCP settings
        if getattr(cfg, "mcp_transport_type", None):
            mcp_transport_type = cfg.mcp_transport_type
        if getattr(cfg, "mcp_executable", None):
            mcp_executable = cfg.mcp_executable
        if getattr(cfg, "mcp_port", None) is not None:
            mcp_port = cfg.mcp_port

        # Create filter if enabled
        result_filter = None
        if getattr(cfg, "enable_filtering", False):
            # Load title list - priority: direct list > JSON file > default JSON file
            title_list = getattr(cfg, "filter_title_list", None)
            
            # If title_list is already a list, use it directly
            if isinstance(title_list, list):
                # Use the list as-is (even if empty - don't fall back to file)
                pass
            elif isinstance(title_list, str):
                # If title_list is a string, check if it's a file path
                if title_list.endswith('.json'):
                    # Treat as file path - don't use as direct list
                    title_list = None
                else:
                    # Parse comma-separated string into list
                    title_list = [t.strip().strip('"\'') for t in title_list.split(",") if t.strip()]
            else:
                # title_list is None or other type - will load from file
                title_list = None
            
            # Only load from file if no direct title list was provided
            if title_list is None:
                title_list_file = getattr(cfg, "filter_title_list_file", None)
                
                # Default to cochrane_titles.json in the filters directory if not specified
                if not title_list_file:
                    # Try multiple possible locations for the default file
                    # File is at: dr-tulu/agent/dr_agent/filters/cochrane_titles.json
                    # This file is at: dr-tulu/agent/workflows/auto_search_sft.py
                    possible_paths = [
                        # Relative to workflows directory: workflows -> agent -> dr_agent -> filters
                        Path(__file__).parent.parent / "dr_agent" / "filters" / "cochrane_titles.json",
                        # Alternative: workflows -> dr_agent -> filters (if structure is different)
                        Path(__file__).parent / "dr_agent" / "filters" / "cochrane_titles.json",
                        # Fallback: try filters directory relative to workflows
                        Path(__file__).parent.parent / "filters" / "cochrane_titles.json",
                    ]
                    
                    for default_json in possible_paths:
                        if default_json.exists():
                            title_list_file = str(default_json)
                            self.logger.info(f"‚úÖ Found default cochrane_titles.json at: {default_json}")
                            break
                    
                    if not title_list_file:
                        self.logger.warning(f"‚ö†Ô∏è Default cochrane_titles.json not found in any of these locations:")
                        for path in possible_paths:
                            self.logger.warning(f"   - {path} (exists: {path.exists()})")
                
                # Load titles from JSON file
                if title_list_file:
                    try:
                        import json
                        json_path = Path(title_list_file)
                        if not json_path.is_absolute():
                            # Try relative to filters directory (multiple possible locations)
                            possible_dirs = [
                                Path(__file__).parent.parent / "dr_agent" / "filters",
                                Path(__file__).parent.parent / "filters",
                                Path(__file__).parent / "dr_agent" / "filters",
                            ]
                            found = False
                            for filters_dir in possible_dirs:
                                candidate = filters_dir / title_list_file
                                if candidate.exists():
                                    json_path = candidate
                                    found = True
                                    break
                            if not found:
                                # Use first option as default
                                json_path = possible_dirs[0] / title_list_file
                        
                        if json_path.exists():
                            with open(json_path, 'r', encoding='utf-8') as f:
                                title_list = json.load(f)
                            if isinstance(title_list, list) and len(title_list) > 0:
                                self.logger.info(f"‚úÖ Loaded {len(title_list)} titles from {json_path}")
                            else:
                                self.logger.warning(f"‚ö†Ô∏è Filter title list file is empty or invalid format: {json_path}")
                                title_list = None
                        else:
                            self.logger.warning(f"‚ö†Ô∏è Filter title list file not found: {json_path}")
                            title_list = None
                    except Exception as e:
                        self.logger.error(f"‚ùå Failed to load filter title list from {title_list_file}: {e}", exc_info=True)
                        title_list = None
                else:
                    self.logger.warning(f"‚ö†Ô∏è Filter title list file not specified and default not found")
                    title_list = None
            
            # Ensure title_list is set (even if None, filter will still work for URL-based filtering)
            if title_list is None:
                self.logger.warning("‚ö†Ô∏è No title filter list loaded - filter will only filter by Cochrane URLs")
            
            result_filter = CochraneResultFilter(
                title_filter_list=title_list,
                source_title=getattr(cfg, "filter_source_title", None),
                publication_date=getattr(cfg, "filter_publication_date", None),
            )
            
            # Log filter configuration
            if title_list:
                self.logger.info(f"‚úÖ Filter configured with {len(title_list)} titles")
                if hasattr(result_filter, 'title_filter') and result_filter.title_filter:
                    self.logger.info("‚úÖ Title filter function created successfully")
                else:
                    self.logger.warning("‚ö†Ô∏è Title filter function was NOT created - check filter initialization")
            else:
                self.logger.info("‚ÑπÔ∏è Filter configured without title list (URL-only filtering)")

        # Search and browse tools (MCP-backed) with unified tool parser
        if cfg.search_tool_name == "serper":
            self.search_tool = SerperSearchTool(
                tool_parser=cfg.tool_parser,
                number_documents_to_search=cfg.number_documents_to_search,
                timeout=cfg.search_timeout,
                name="snippet_search",  # <- to test this v20250824 model, we need to set the tool name in a hacky way.
                transport_type=mcp_transport_type,
                mcp_executable=mcp_executable,
                mcp_port=mcp_port,
                result_filter=result_filter,
            )

            self.search_tool2 = SerperSearchTool(
                tool_parser=cfg.tool_parser,
                number_documents_to_search=cfg.number_documents_to_search,
                timeout=cfg.search_timeout,
                name="google_search",
                transport_type=mcp_transport_type,
                mcp_executable=mcp_executable,
                mcp_port=mcp_port,
                result_filter=result_filter,
            )
        elif cfg.search_tool_name == "s2":
            self.search_tool = SemanticScholarSnippetSearchTool(
                tool_parser=cfg.tool_parser,
                number_documents_to_search=cfg.number_documents_to_search,
                timeout=cfg.search_timeout,
                name="snippet_search",
                transport_type=mcp_transport_type,
                mcp_executable=mcp_executable,
                mcp_port=mcp_port,
                result_filter=result_filter,
            )

            self.search_tool2 = SerperSearchTool(
                tool_parser=cfg.tool_parser,
                number_documents_to_search=cfg.number_documents_to_search,
                timeout=cfg.search_timeout,
                name="google_search",
                transport_type=mcp_transport_type,
                mcp_executable=mcp_executable,
                mcp_port=mcp_port,
                result_filter=result_filter,
            )
        elif cfg.search_tool_name == "s2-only":
            self.search_tool = SemanticScholarSnippetSearchTool(
                tool_parser=cfg.tool_parser,
                number_documents_to_search=cfg.number_documents_to_search,
                timeout=cfg.search_timeout,
                name="snippet_search",
                transport_type=mcp_transport_type,
                mcp_executable=mcp_executable,
                mcp_port=mcp_port,
                result_filter=result_filter,
            )

            self.search_tool2 = SemanticScholarSnippetSearchTool(
                tool_parser=cfg.tool_parser,
                number_documents_to_search=cfg.number_documents_to_search,
                timeout=cfg.search_timeout,
                name="google_search",
                transport_type=mcp_transport_type,
                mcp_executable=mcp_executable,
                mcp_port=mcp_port,
                result_filter=result_filter,
            )
        else:
            raise ValueError(f"Invalid search tool name: {cfg.search_tool_name}")

        if cfg.browse_tool_name == "serper":
            self.browse_tool = SerperBrowseTool(
                tool_parser=cfg.tool_parser,
                max_pages_to_fetch=cfg.browse_max_pages_to_fetch,
                timeout=cfg.browse_timeout,
                name="browse_webpage",
                transport_type=mcp_transport_type,
                mcp_executable=mcp_executable,
                mcp_port=mcp_port,
                result_filter=result_filter,
            )
        elif cfg.browse_tool_name == "crawl4ai":
            self.browse_tool = Crawl4AIBrowseTool(
                tool_parser=cfg.tool_parser,
                max_pages_to_fetch=cfg.browse_max_pages_to_fetch,
                timeout=cfg.browse_timeout,
                name="browse_webpage",
                transport_type=mcp_transport_type,
                mcp_executable=mcp_executable,
                mcp_port=mcp_port,
                context_chars=cfg.browse_context_char_length,
                use_docker_version=cfg.crawl4ai_use_docker_version,
                use_ai2_config=cfg.crawl4ai_use_ai2_config,
                result_filter=result_filter,
            )
        elif cfg.browse_tool_name == "jina":
            self.browse_tool = JinaBrowseTool(
                tool_parser=cfg.tool_parser,
                timeout=cfg.browse_timeout,
                name="browse_webpage",
                transport_type=mcp_transport_type,
                mcp_executable=mcp_executable,
                mcp_port=mcp_port,
                result_filter=result_filter,
            )
        elif cfg.browse_tool_name is None:
            self.browse_tool = NoBrowseTool(
                tool_parser=cfg.tool_parser,
                name="browse_webpage",
            )
        else:
            raise ValueError(f"Invalid browse tool name: {cfg.browse_tool_name}")
        print("Using browse tool: ", self.browse_tool)

        if cfg.use_browse_agent:
            with LLMToolClient(
                model_name=cfg.browse_agent_model_name,
                tokenizer_name=cfg.browse_agent_tokenizer_name,
                base_url=cfg.browse_agent_base_url,
                api_key=cfg.browse_agent_api_key,
            ) as client:
                self.browse_agent = WebPageReaderAgentV2(client=client).as_tool(
                    max_tokens=cfg.browse_agent_max_tokens,
                    temperature=cfg.browse_agent_temperature,
                )
                self.composed_browse_tool = ChainedTool(
                    [self.browse_tool, self.browse_agent],
                    name="browse_webpage",
                    tool_parser=cfg.tool_parser,
                    output_formatting="last",
                )
        else:
            self.composed_browse_tool = self.browse_tool

        with LLMToolClient(
            model_name=cfg.search_agent_model_name,
            tokenizer_name=cfg.search_agent_tokenizer_name,
            base_url=cfg.search_agent_base_url,
            api_key=cfg.search_agent_api_key,
        ) as client:
            self.search_agent = SearchAgent(
                client=client,
                tools=[self.search_tool, self.search_tool2, self.composed_browse_tool],
                prompt_version=cfg.prompt_version,
                use_research_assistant_prompt=getattr(cfg, "use_research_assistant_prompt", False),
            )
            self.answer_agent = AnswerAgent(
                client=client,
                prompt_version=cfg.prompt_version,
            )

    async def __call__(
        self,
        problem: str,
        dataset_name: Optional[str] = None,
        messages: Optional[List[Dict[str, str]]] = None,
        verbose: bool = True,
        search_callback: Optional[Any] = None,
        step_callback: Optional[Any] = None,
    ) -> Dict[str, Any]:
        cfg = self.configuration
        assert cfg is not None

        # Extract history and problem from messages if provided
        history = []
        if messages:
            # Find the last user message as the problem
            last_user_idx = -1
            for i in range(len(messages) - 1, -1, -1):
                if messages[i]["role"] == "user":
                    last_user_idx = i
                    break

            if last_user_idx != -1:
                problem = messages[last_user_idx]["content"]
                history = messages[:last_user_idx]
            else:
                # Fallback if no user message found (shouldn't happen ideally)
                history = messages

        # import litellm
        # litellm._turn_on_debug()

        # Reset filtered URLs at the start of a new query
        # This ensures each query starts with a clean slate for URL-based filtering
        # (matches sciconbench_code pattern in mcp_client.py line 303)
        try:
            # Try sciconbench_code utils first (if available)
            from mcp_client.utils.utils import reset_filtered_urls
            reset_filtered_urls()
            self.logger.debug("‚úÖ Reset filtered URLs at start of new query (sciconbench_code utils)")
        except ImportError:
            # Try agent utils as fallback
            try:
                from dr_agent.utils.utils import reset_filtered_urls
                reset_filtered_urls()
                self.logger.debug("‚úÖ Reset filtered URLs at start of new query (agent utils)")
            except ImportError:
                # If utils module not available, skip URL reset (filter will still work)
                self.logger.debug("‚ÑπÔ∏è URL reset not available (utils module not found)")
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Failed to reset filtered URLs: {e}")

        # Set the question for the browse agent
        # TODO: This is a bit hectic and hacky, but it works for now
        # The problem: it uses a bad way to enable the runtime dynamics
        if isinstance(self.composed_browse_tool, ChainedTool):
            browse_tool = self.composed_browse_tool.tools[0]
            browse_tool.bm25_query = problem
            browse_agent = self.composed_browse_tool.tools[-1]
            browse_agent.agent.question = problem
        else:
            browse_tool = self.composed_browse_tool
            browse_tool.bm25_query = problem

        results = await self.search_agent(
            question=problem,
            dataset_name=dataset_name,
            history=history,
            max_tokens=cfg.search_agent_max_tokens,
            temperature=cfg.search_agent_temperature,
            max_tool_calls=cfg.search_agent_max_tool_calls,
            verbose=verbose,
            on_step_callback=step_callback,
        )

        if search_callback:
            if asyncio.iscoroutinefunction(search_callback):
                await search_callback(results)
            else:
                search_callback(results)

        browsed_links = []
        searched_links = []
        total_tool_calls = 0
        failed_tool_calls = 0
        failed_tool_call_errors = []
        for tool_output in results.tool_calls:
            total_tool_calls += 1
            if tool_output.error != "":
                failed_tool_calls += 1
                failed_tool_call_errors.append(tool_output.error)

            if tool_output.tool_name in ["snippet_search", "google_search"]:
                searched_links.extend(
                    [document.url for document in tool_output.documents]
                )

            if tool_output.tool_name == "browse_webpage":
                if isinstance(self.composed_browse_tool, ChainedTool):
                    if tool_output.raw_output is None:
                        continue
                    if chained_tool_outputs := tool_output.raw_output.get(
                        "tool_outputs"
                    ):
                        for document in chained_tool_outputs[0].documents:
                            if document.url:
                                browsed_links.append(document.url)
                else:
                    if hasattr(tool_output, "documents"):
                        for document in tool_output.documents:
                            if document.url:
                                browsed_links.append(document.url)
                    else:
                        print(
                            f"Warning: browse_webpage tool output has no documents: {tool_output}"
                        )

        browsed_links = list(set(browsed_links))
        searched_links = list(set(searched_links))

        if "<answer>" in results.generated_text:
            return {
                "final_response": self.search_agent.postprocess_output(results),
                "full_traces": results,
                "browsed_links": browsed_links,
                "searched_links": searched_links,
                "total_tool_calls": total_tool_calls,
                "total_failed_tool_calls": failed_tool_calls,
                "failed_tool_call_errors": failed_tool_call_errors,
            }

        answer = await self.answer_agent(
            question=problem,
            history=results.generated_text,
            dataset_name=dataset_name,
            additional_instructions="Now please generate an based on the search results by far:",
            generation_prefix="<answer>",
            max_tokens=cfg.search_agent_max_tokens,
            temperature=cfg.search_agent_temperature,
            verbose=verbose,
            on_step_callback=step_callback,
        )

        if verbose:
            print(results)  # noqa: T201

        answer.tool_calls = [results.model_dump()]

        return {
            "final_response": self.answer_agent.postprocess_output(answer),
            "full_traces": answer,
            "browsed_links": browsed_links,
            "searched_links": searched_links,
        }


if __name__ == "__main__":
    AutoReasonSearchWorkflow.app()
